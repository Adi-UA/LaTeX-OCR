#!/bin/bash

# Create Virtual Environment
echo "Creating virtual environment..."
python3 -m venv venv
source venv/bin/activate

# Install the required packages
echo "Installing required packages..."
pip3 install pix2tex[train] gpustat opencv-python-headless wandb gdown

echo Removing old data
rm -rf dataset images

echo Fetching data
mkdir -p dataset/data
mkdir images

gdown -O dataset/data/crohme.zip --id 13vjxGYrFCuYnwgDIUqkxsNGKk__D_sOM
gdown -O dataset/data/pdf.zip --id 176PKaCUDWmTJdQwc-OfkO0y8t4gLsIvQ
gdown -O dataset/data/pdfmath.txt --id 1QUjX6PFWPa-HBWdcY-7bA5TRVUnbyS1D

echo Unzipping data
cd dataset/data
unzip -q crohme.zip 
unzip -q pdf.zip 
# split handwritten data into val set and train set
cd images
mkdir ../valimages
ls | shuf -n 1000 | xargs -i mv {} ../valimages
cd ../../..


echo Combining files and generating tokenizer
cat dataset/data/CROHME_math.txt > dataset/data/all_math.txt
cat dataset/data/pdfmath.txt >> dataset/data/all_math.txt
python3 -m pix2tex.dataset.dataset --equations dataset/data/all_math.txt --vocab-size 8000 --out custom/og_tokenizer.json

echo Reorganizing data for training
python -m pix2tex.dataset.dataset -i dataset/data/images dataset/data/train -e dataset/data/CROHME_math.txt dataset/data/pdfmath.txt -o dataset/data/train.pkl -t custom/og_tokenizer.json
python -m pix2tex.dataset.dataset -i dataset/data/valimages dataset/data/val -e dataset/data/CROHME_math.txt dataset/data/pdfmath.txt -o dataset/data/val.pkl -t custom/og_tokenizer.json

echo Training
python -m pix2tex.train --config custom/reproduce_config.yaml